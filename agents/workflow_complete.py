"""
å®Œæ•´ LifeOS æ™ºèƒ½ä½“å·¥ä½œæµ - æ”¹è¿›ç‰ˆ
ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰ prompt éƒ½è¢«æ­£ç¡®ä½¿ç”¨
"""

import json
import os
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from agents.state import AgentState
from agents.prompts_complete import (
    complete_intent_recognition_prompt,
    enhanced_task_extraction_prompt,
    personalization_prompt,  # â† æ–°å¢ï¼šå°†è¢«æ­£ç¡®ä½¿ç”¨
    emotion_support_prompt,
    habit_management_prompt,
    goal_planning_prompt,
    reflection_prompt
)
from agents.tools_complete import get_complete_tools
from agents.conversation_manager import ConversationManager

# å°è¯•å¯¼å…¥è…¾è®¯æ··å…ƒ
try:
    from agents.hunyuan_llm import HunyuanLLM
    HUNYUAN_AVAILABLE = True
except ImportError:
    HUNYUAN_AVAILABLE = False
    print("âš ï¸ è…¾è®¯äº‘ SDK æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æ··å…ƒæ¨¡å‹")


class CompleteLifeOSWorkflow:
    """
    å®Œæ•´ LifeOS æ™ºèƒ½ä½“å·¥ä½œæµ
    """
    
    def __init__(
        self,
        llm: Optional[Union[ChatOpenAI, HunyuanLLM]] = None,
        db_path: str = "lifeos_data.db",
        enable_conversation_memory: bool = True
    ):
        self.llm = llm
        self.db_path = db_path
        self.tools = get_complete_tools(db_path)
        self.conversation_manager = ConversationManager(db_path) if enable_conversation_memory else None
        self.workflow_app = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºå®Œæ•´å·¥ä½œæµå›¾"""
        workflow = StateGraph(AgentState)
        
        # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
        workflow.add_node("intent_recognition", self._intent_recognition_node)
        workflow.add_node("task_processing", self._task_processing_node)
        workflow.add_node("emotion_support", self._emotion_support_node)
        workflow.add_node("habit_management", self._habit_management_node)
        workflow.add_node("goal_planning", self._goal_planning_node)
        workflow.add_node("reflection_guide", self._reflection_guide_node)
        workflow.add_node("casual_response", self._casual_response_node)
        workflow.add_node("personalization", self._personalization_node)  # â† æ–°å¢èŠ‚ç‚¹
        workflow.add_node("output_generation", self._output_generation_node)
        
        # è®¾ç½®å…¥å£
        workflow.set_entry_point("intent_recognition")
        
        # æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "intent_recognition",
            self._route_by_intent,
            {
                "task_management": "task_processing",
                "emotion_support": "emotion_support",
                "habit_tracking": "habit_management",
                "goal_setting": "goal_planning",
                "reflection": "reflection_guide",
                "casual_chat": "casual_response"
            }
        )
        
        # ä»»åŠ¡å¤„ç†åå¯é€‰æ‹©æ€§è¿›è¡Œä¸ªæ€§åŒ–å¢å¼º
        workflow.add_conditional_edges(
            "task_processing",
            self._should_personalize,
            {
                "personalize": "personalization",
                "skip": "output_generation"
            }
        )
        
        # ä¸ªæ€§åŒ–ååˆ°è¾“å‡º
        workflow.add_edge("personalization", "output_generation")
        
        # å…¶ä»–è·¯å¾„ç›´æ¥åˆ°è¾“å‡º
        for node in ["emotion_support", "habit_management",
                     "goal_planning", "reflection_guide", "casual_response"]:
            workflow.add_edge(node, "output_generation")
        
        workflow.add_edge("output_generation", END)
        
        return workflow.compile()
    
    def _should_personalize(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸ªæ€§åŒ–å¢å¼º"""
        # å¦‚æœæœ‰å¤šä¸ªä»»åŠ¡ä¸”æœ‰ç”¨æˆ·ç”»åƒï¼Œåˆ™è¿›è¡Œä¸ªæ€§åŒ–
        tasks = state.get("analyzed_tasks", [])
        if len(tasks) >= 2:
            return "personalize"
        return "skip"
    
    def _parse_json_response(self, content: str) -> Dict[str, Any]:
        """è§£æ JSON å“åº”ï¼ˆæ”¹è¿›ç‰ˆï¼Œå¤„ç† Markdown ä»£ç å—ï¼‰"""
        try:
            # ç§»é™¤å¯èƒ½çš„ Markdown ä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # å°è¯•æå– JSON
            start = content.find('{')
            end = content.rfind('}') + 1
            if start >= 0 and end > start:
                json_str = content[start:end]
                return json.loads(json_str)
            return {}
        except Exception as e:
            print(f"   âš ï¸ JSON è§£æå¤±è´¥: {e}")
            print(f"   ğŸ“„ åŸå§‹å†…å®¹: {content[:200]}...")
            return {}
    
    def _build_conversation_summary(self, history: List[Dict]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not history:
            return "ï¼ˆè¿™æ˜¯æ–°å¯¹è¯çš„å¼€å§‹ï¼‰"
        
        recent = history[-3:]  # æœ€è¿‘3è½®ï¼ˆå¢åŠ ä¸Šä¸‹æ–‡ï¼‰
        if len(recent) == 0:
            return "ï¼ˆè¿™æ˜¯æ–°å¯¹è¯çš„å¼€å§‹ï¼‰"
        
        summary = []
        for i, turn in enumerate(recent, 1):
            user_msg = turn.get('user_message', '')
            assistant_msg = turn.get('assistant_message', '')
            intent = turn.get('intent', 'unknown')
            
            summary.append(f"ç¬¬{i}è½®:")
            summary.append(f"  ç”¨æˆ·: {user_msg[:50]}...")
            summary.append(f"  æ„å›¾: {intent}")
            summary.append(f"  å›å¤: {assistant_msg[:60]}...")
        
        return "\n".join(summary)
    
    def _extract_user_profile(self, conversation_history: List[Dict]) -> str:
        """ä»å¯¹è¯å†å²ä¸­æå–ç”¨æˆ·ç”»åƒï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰"""
        if not conversation_history:
            return "æš‚æ— ç”¨æˆ·ç”»åƒæ•°æ®"
        
        # ç®€å•æå–ï¼šç»Ÿè®¡ç”¨æˆ·çš„è¡Œä¸ºåå¥½
        task_count = sum(1 for h in conversation_history if h.get('intent') == 'task_management')
        emotion_count = sum(1 for h in conversation_history if h.get('intent') == 'emotion_support')
        goal_count = sum(1 for h in conversation_history if h.get('intent') == 'goal_setting')
        
        profile = []
        if task_count > 2:
            profile.append("å·¥ä½œé£æ ¼: ä»»åŠ¡å¯¼å‘å‹ï¼ˆå–œæ¬¢æ•´ç†å’Œè§„åˆ’ï¼‰")
        if emotion_count > 1:
            profile.append("å‹åŠ›åº”å¯¹: æƒ…ç»ªæŠ’å‘å‹ï¼ˆéœ€è¦æƒ…æ„Ÿæ”¯æŒï¼‰")
        if goal_count > 1:
            profile.append("ç›®æ ‡ç‰¹ç‚¹: ç›®æ ‡é©±åŠ¨å‹ï¼ˆé‡è§†é•¿æœŸè§„åˆ’ï¼‰")
        
        return "\n".join(profile) if profile else "æš‚æ— æ˜æ˜¾åå¥½"
    
    # =========================================================================
    # èŠ‚ç‚¹å‡½æ•°
    # =========================================================================
    
    def _intent_recognition_node(self, state: AgentState) -> Dict:
        """æ„å›¾è¯†åˆ«èŠ‚ç‚¹ - ä½¿ç”¨ complete_intent_recognition_prompt"""
        print("ğŸ” [æ„å›¾è¯†åˆ«] è°ƒç”¨ LLM åˆ†æ...")
        
        user_input = state["user_input"]
        conversation_history = state.get("conversation_history", [])
        
        conv_summary = self._build_conversation_summary(conversation_history)
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ complete_intent_recognition_prompt
                prompt = complete_intent_recognition_prompt.format_messages(
                    user_input=user_input,
                    conversation_summary=conv_summary
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                intent = result.get("intent", "casual_chat")
                confidence = result.get("confidence", 0.7)
                reasoning = result.get("reasoning", "LLM åˆ†æ")
                context_continuation = result.get("context_continuation", False)
                
                print(f"   âœ“ æ„å›¾: {intent} (ç½®ä¿¡åº¦: {confidence:.2f})")
                print(f"   ğŸ’¡ æ¨ç†: {reasoning[:60]}...")
                if context_continuation:
                    print(f"   ğŸ”— æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡å»¶ç»­")
                
                return {
                    "intent": intent,
                    "confidence": confidence,
                    "context_continuation": context_continuation,
                    "processing_steps": [f"ğŸ¤– æ„å›¾è¯†åˆ«: {intent} - {reasoning}"]
                }
            
            except Exception as e:
                print(f"   âš ï¸ LLM è°ƒç”¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§ï¼šç®€å•è§„åˆ™åŒ¹é…
        intent = self._fallback_intent_detection(user_input)
        return {
            "intent": intent,
            "confidence": 0.6,
            "context_continuation": False,
            "processing_steps": [f"è§„åˆ™åŒ¹é…: {intent}"]
        }
    
    def _fallback_intent_detection(self, text: str) -> str:
        """é™çº§çš„æ„å›¾æ£€æµ‹"""
        text_lower = text.lower()
        
        if any(k in text_lower for k in ['ä¹ æƒ¯', 'åšæŒ', 'æ‰“å¡']):
            return "habit_tracking"
        elif any(k in text_lower for k in ['ç›®æ ‡', 'æƒ³è¦', 'è®¡åˆ’', 'å®ç°', 'å­¦ä¹ ']):
            return "goal_setting"
        elif any(k in text_lower for k in ['æ€»ç»“', 'åæ€', 'å›é¡¾', 'å¤ç›˜']):
            return "reflection"
        elif any(k in text_lower for k in ['ç´¯', 'ç„¦è™‘', 'å‹åŠ›', 'å´©æºƒ', 'ç–²æƒ«']):
            return "emotion_support"
        elif any(k in text_lower for k in ['ä»»åŠ¡', 'è¦åš', 'æ•´ç†', 'å¾…åŠ', 'å®‰æ’']):
            return "task_management"
        else:
            return "casual_chat"
    
    def _task_processing_node(self, state: AgentState) -> Dict:
        """ä»»åŠ¡å¤„ç†èŠ‚ç‚¹ - ä½¿ç”¨ enhanced_task_extraction_prompt"""
        print("ğŸ“‹ [ä»»åŠ¡å¤„ç†] æå–å¹¶åˆ†æä»»åŠ¡...")
        
        user_input = state["user_input"]
        conv_summary = self._build_conversation_summary(
            state.get("conversation_history", [])
        )
        
        # å¤„ç†ä¸Šä¸‹æ–‡å»¶ç»­ï¼ˆå¦‚"ç¬¬äºŒæ­¥å‘¢"ï¼‰
        context_continuation = state.get("context_continuation", False)
        if context_continuation and len(user_input) < 20:
            print("   ğŸ” æ£€æµ‹åˆ°å»¶ç»­æ€§æé—®ï¼Œä»å¯¹è¯å†å²ä¸­æå–ä»»åŠ¡ä¸Šä¸‹æ–‡...")
            user_input_with_context = f"{conv_summary}\n\nå½“å‰é—®é¢˜ï¼š{user_input}"
        else:
            user_input_with_context = user_input
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ enhanced_task_extraction_prompt
                # æ³¨æ„ï¼šè¿™ä¸ª prompt åªæ¥å— user_input å‚æ•°
                prompt = enhanced_task_extraction_prompt.format_messages(
                    user_input=user_input_with_context
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                tasks = result.get("tasks", [])
                priority_analysis = result.get("priority_analysis", {})
                suggestions = result.get("suggestions", [])
                total_count = result.get("total_count", len(tasks))
                
                # æŒ‰ä¼˜å…ˆçº§æ’åºä»»åŠ¡
                priority_order = {'high': 1, 'medium': 2, 'low': 3, '': 4}
                tasks.sort(key=lambda t: priority_order.get(t.get('priority', '').lower(), 4))
                
                print(f"   âœ“ æå–åˆ° {len(tasks)} ä¸ªä»»åŠ¡")
                print(f"   ğŸ“Š ä¼˜å…ˆçº§åˆ†æ: {priority_analysis}")
                
                if len(tasks) == 0:
                    # æ ¹æ®å»ºè®®ç”Ÿæˆæ™ºèƒ½å›åº”
                    fallback_suggestions = suggestions if suggestions else [
                        "æœªæ£€æµ‹åˆ°å…·ä½“ä»»åŠ¡ã€‚ä½ å¯ä»¥å‘Šè¯‰æˆ‘éœ€è¦å¤„ç†çš„äº‹æƒ…ï¼Œæˆ‘ä¼šå¸®ä½ æ•´ç†ã€‚"
                    ]
                    return {
                        "analyzed_tasks": [],
                        "final_output": "\n".join(fallback_suggestions),
                        "processing_steps": ["ğŸ“ æœªæ£€æµ‹åˆ°ä»»åŠ¡ï¼Œæä¾›å¼•å¯¼å»ºè®®"]
                    }
                
                # æ„å»ºè¾“å‡º
                output_parts = []
                output_parts.append(f"å¥½çš„ï¼æˆ‘å¸®ä½ æ•´ç†äº† {total_count} ä¸ªä»»åŠ¡ï¼š\n")
                
                # ä»»åŠ¡åˆ—è¡¨ï¼ˆå¸¦ä¼˜å…ˆçº§æ ‡è¯†ï¼‰
                for i, t in enumerate(tasks[:5], 1):
                    title = t.get('title', 'ä»»åŠ¡')
                    priority = t.get('priority', '').lower()
                    deadline = t.get('deadline', '')
                    estimated_time = t.get('estimated_time', '')
                    
                    priority_icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(priority, 'âšª')
                    priority_text = {'high': 'é«˜ä¼˜å…ˆçº§', 'medium': 'ä¸­ä¼˜å…ˆçº§', 'low': 'ä½ä¼˜å…ˆçº§'}.get(priority, '')
                    
                    task_line = f"{i}. {priority_icon} {title}"
                    if priority_text:
                        task_line += f" ({priority_text})"
                    if deadline:
                        task_line += f" | â° {deadline}"
                    if estimated_time:
                        task_line += f" | é¢„è®¡ {estimated_time}"
                    
                    output_parts.append(task_line)
                
                # ä¼˜å…ˆçº§åˆ†æ
                urgent_count = priority_analysis.get('urgent_count', 0)
                important_first = priority_analysis.get('important_first', '')
                
                if urgent_count > 0:
                    output_parts.append(f"\nğŸ”´ æœ‰ {urgent_count} ä¸ªé«˜ä¼˜å…ˆçº§ä»»åŠ¡éœ€è¦ä¼˜å…ˆå¤„ç†")
                    if important_first:
                        output_parts.append(f"ğŸ’¡ å»ºè®®å…ˆä»ã€Œ{important_first}ã€å¼€å§‹")
                
                # æ‰§è¡Œå»ºè®®
                if suggestions:
                    output_parts.append("\nğŸ’¡ æ‰§è¡Œå»ºè®®ï¼š")
                    for s in suggestions[:3]:
                        output_parts.append(f"â€¢ {s}")
                
                final_output = "\n".join(output_parts)
                
                print(f"   âœ“ ä»»åŠ¡åˆ†æå®Œæˆ")
                
                return {
                    "analyzed_tasks": tasks,
                    "priority_analysis": priority_analysis,
                    "final_output": final_output,
                    "processing_steps": [
                        f"ğŸ“ ä»»åŠ¡æå–: {len(tasks)}ä¸ª",
                        f"ğŸ“Š ä¼˜å…ˆçº§åˆ†æ: {urgent_count}ä¸ªç´§æ€¥ä»»åŠ¡",
                        "ğŸ’¡ ç”Ÿæˆæ‰§è¡Œå»ºè®®"
                    ]
                }
                
            except Exception as e:
                print(f"   âš ï¸ ä»»åŠ¡å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§å¤„ç†
        lines = [l.strip() for l in user_input.split('\n') if l.strip()]
        task_list = "\n".join([f"{i+1}. {l}" for i, l in enumerate(lines[:5])])
        return {
            "analyzed_tasks": [{"title": l, "priority": "medium"} for l in lines[:5]],
            "final_output": f"æˆ‘å¸®ä½ æ•´ç†äº†ä»»åŠ¡ï¼š\n\n{task_list}\n\nğŸ’¡ å»ºè®®å…ˆä»æœ€é‡è¦çš„å¼€å§‹ï¼",
            "processing_steps": ["ç®€å•ä»»åŠ¡æ‹†åˆ†ï¼ˆé™çº§æ¨¡å¼ï¼‰"]
        }
    
    def _personalization_node(self, state: AgentState) -> Dict:
        """ä¸ªæ€§åŒ–å¢å¼ºèŠ‚ç‚¹ - ä½¿ç”¨ personalization_prompt"""
        print("ğŸ¨ [ä¸ªæ€§åŒ–] æ ¹æ®ç”¨æˆ·ç”»åƒä¼˜åŒ–å»ºè®®...")
        
        if not self.llm:
            print("   âš ï¸ æ—  LLMï¼Œè·³è¿‡ä¸ªæ€§åŒ–")
            return {}
        
        try:
            # æå–ç”¨æˆ·ç”»åƒ
            conversation_history = state.get("conversation_history", [])
            user_profile = self._extract_user_profile(conversation_history)
            
            # æ„å»ºä»»åŠ¡åˆ—è¡¨
            tasks = state.get("analyzed_tasks", [])
            current_tasks = "\n".join([
                f"- {t.get('title', 'ä»»åŠ¡')} (ä¼˜å…ˆçº§: {t.get('priority', 'medium')})"
                for t in tasks[:5]
            ])
            
            # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
            conv_history_text = self._build_conversation_summary(conversation_history)
            
            # âœ… æ­£ç¡®ä½¿ç”¨ personalization_prompt
            prompt = personalization_prompt.format_messages(
                user_profile=user_profile,
                current_tasks=current_tasks,
                conversation_history=conv_history_text
            )
            
            response = self.llm.invoke(prompt)
            result = self._parse_json_response(response.content)
            
            personalized_suggestions = result.get("personalized_suggestions", [])
            adapted_timeline = result.get("adapted_timeline", "")
            motivation_style = result.get("motivation_style", "ç›®æ ‡é©±åŠ¨å‹")
            
            print(f"   âœ“ ä¸ªæ€§åŒ–å®Œæˆ (æ¿€åŠ±æ–¹å¼: {motivation_style})")
            
            # å¢å¼ºåŸæœ‰è¾“å‡º
            enhanced_output = state.get("final_output", "")
            if personalized_suggestions:
                enhanced_output += "\n\nğŸ¯ æ ¹æ®ä½ çš„ä¹ æƒ¯å®šåˆ¶å»ºè®®ï¼š"
                for s in personalized_suggestions[:3]:
                    enhanced_output += f"\nâ€¢ {s}"
            
            if adapted_timeline:
                enhanced_output += f"\n\nâ° æ¨èæ—¶é—´å®‰æ’ï¼š\n{adapted_timeline}"
            
            return {
                "final_output": enhanced_output,
                "processing_steps": state.get("processing_steps", []) + [
                    f"ğŸ¨ ä¸ªæ€§åŒ–å¢å¼º ({motivation_style})"
                ]
            }
            
        except Exception as e:
            print(f"   âš ï¸ ä¸ªæ€§åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _emotion_support_node(self, state: AgentState) -> Dict:
        """æƒ…ç»ªæ”¯æŒèŠ‚ç‚¹ - ä½¿ç”¨ emotion_support_prompt"""
        print("ğŸ’š [æƒ…ç»ªæ”¯æŒ] ç”Ÿæˆæ¸©æš–å›åº”...")
        
        user_input = state["user_input"]
        conv_summary = self._build_conversation_summary(
            state.get("conversation_history", [])
        )
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ emotion_support_prompt
                prompt = emotion_support_prompt.format_messages(
                    user_input=user_input,
                    conversation_summary=conv_summary
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                empathy_response = result.get("empathy_response", "æˆ‘ç†è§£ä½ çš„æ„Ÿå—")
                suggestions = result.get("suggestions", [])
                quick_actions = result.get("quick_actions", [])
                tone = result.get("tone", "æ¸©æš–")
                
                print(f"   âœ“ å›åº”è¯­æ°”: {tone}")
                
                # æ„å»ºè¾“å‡º
                output_parts = [empathy_response]
                
                if suggestions:
                    output_parts.append("\nğŸ’¡ ä¸€äº›æƒ³æ³•ï¼š")
                    for s in suggestions[:2]:
                        output_parts.append(f"â€¢ {s}")
                
                if quick_actions:
                    output_parts.append("\nğŸŒŸ å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥è¯•è¯•ï¼š")
                    for a in quick_actions[:2]:
                        output_parts.append(f"â€¢ {a}")
                
                final_output = "\n".join(output_parts)
                
                return {
                    "final_output": final_output,
                    "processing_steps": [f"ğŸ’š æƒ…ç»ªæ”¯æŒ (è¯­æ°”: {tone})"]
                }
                
            except Exception as e:
                print(f"   âš ï¸ æƒ…ç»ªæ”¯æŒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§å›åº”
        return {
            "final_output": "æˆ‘ç†è§£ä½ ç°åœ¨çš„æ„Ÿå—ã€‚è¦ä¸è¦å…ˆä¼‘æ¯ä¸€ä¸‹ï¼Œç„¶åæˆ‘ä»¬ä¸€èµ·æ•´ç†æ€è·¯ï¼Ÿ",
            "processing_steps": ["ğŸ’š ç®€å•æƒ…ç»ªå›åº”"]
        }
    
    def _habit_management_node(self, state: AgentState) -> Dict:
        """ä¹ æƒ¯ç®¡ç†èŠ‚ç‚¹ - ä½¿ç”¨ habit_management_prompt"""
        print("ğŸ¯ [ä¹ æƒ¯ç®¡ç†] å¤„ç†ä¹ æƒ¯ç›¸å…³è¯·æ±‚...")
        
        user_input = state["user_input"]
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ habit_management_prompt
                prompt = habit_management_prompt.format_messages(
                    user_input=user_input
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                habit_plan = result.get("habit_plan", {})
                motivation = result.get("motivation_message", "")
                
                print(f"   âœ“ ä¹ æƒ¯è®¡åˆ’: {habit_plan.get('habit_name', 'æ–°ä¹ æƒ¯')}")
                
                # æ„å»ºè¾“å‡º
                output_parts = ["å¥½çš„ï¼Œå¸®ä½ è®¾è®¡ä¹ æƒ¯è®¡åˆ’ï¼š\n"]
                output_parts.append(f"ğŸ“Œ **ä¹ æƒ¯åç§°**: {habit_plan.get('habit_name', 'æ–°ä¹ æƒ¯')}")
                output_parts.append(f"â° **é¢‘ç‡**: {habit_plan.get('frequency', 'æ¯å¤©')}")
                output_parts.append(f"ğŸ¯ **è§¦å‘æ¡ä»¶**: {habit_plan.get('trigger', 'è®¾å®šä¸€ä¸ªè§¦å‘æ¡ä»¶')}")
                output_parts.append(f"ğŸ **å°å¥–åŠ±**: {habit_plan.get('reward', 'å®Œæˆåå¥–åŠ±è‡ªå·±')}")
                output_parts.append(f"ğŸŒ± **ä»å°å¼€å§‹**: {habit_plan.get('start_small', 'ä¸€æ­¥ä¸€æ­¥æ¥')}")
                output_parts.append(f"ğŸ“Š **è¿½è¸ªæ–¹å¼**: {habit_plan.get('tracking_method', 'æ¯æ—¥æ‰“å¡')}")
                
                if motivation:
                    output_parts.append(f"\nğŸ’ª {motivation}")
                
                final_output = "\n".join(output_parts)
                
                return {
                    "final_output": final_output,
                    "processing_steps": ["ğŸ¯ ä¹ æƒ¯è®¡åˆ’è®¾è®¡"]
                }
                
            except Exception as e:
                print(f"   âš ï¸ ä¹ æƒ¯ç®¡ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§
        return {
            "final_output": "å¥½çš„ï¼è¦å…»æˆæ–°ä¹ æƒ¯ï¼Œå»ºè®®ï¼š\n1. ä»å°ç›®æ ‡å¼€å§‹\n2. è®¾å®šå›ºå®šæ—¶é—´\n3. è®°å½•æ‰“å¡è¿›åº¦",
            "processing_steps": ["ğŸ¯ ç®€å•ä¹ æƒ¯å»ºè®®"]
        }
    
    def _goal_planning_node(self, state: AgentState) -> Dict:
        """ç›®æ ‡è§„åˆ’èŠ‚ç‚¹ - ä½¿ç”¨ goal_planning_prompt"""
        print("ğŸ¯ [ç›®æ ‡è§„åˆ’] æ‹†è§£ç›®æ ‡...")
        
        user_input = state["user_input"]
        conversation_history = state.get("conversation_history", [])
        conv_summary = self._build_conversation_summary(conversation_history)
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ goal_planning_prompt
                prompt = goal_planning_prompt.format_messages(
                    user_input=user_input,
                    conversation_summary=conv_summary
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå»¶ç»­æ€§å›ç­”
                is_continuation = result.get("is_continuation", False)
                
                if is_continuation:
                    # å¤„ç†"ç¬¬Xæ­¥"ç±»å‹çš„é—®é¢˜
                    step_num = result.get("step_number", 2)
                    action = result.get("action", "ç»§ç»­è¡ŒåŠ¨")
                    details = result.get("details", "")
                    time_req = result.get("time_required", "")
                    result_exp = result.get("expected_result", "")
                    
                    print(f"   âœ“ å»¶ç»­ç›®æ ‡: ç¬¬{step_num}æ­¥")
                    
                    output_parts = [f"ğŸš€ **ç¬¬{step_num}æ­¥**:\n"]
                    output_parts.append(f"ğŸ“ **è¡ŒåŠ¨**: {action}\n")
                    if details:
                        output_parts.append(f"ğŸ’¡ **è¯¦ç»†è¯´æ˜**: {details}\n")
                    if time_req:
                        output_parts.append(f"â±ï¸ **é¢„è®¡è€—æ—¶**: {time_req}")
                    if result_exp:
                        output_parts.append(f"âœ¨ **é¢„æœŸæˆæœ**: {result_exp}")
                    
                    return {
                        "final_output": "\n".join(output_parts),
                        "processing_steps": [f"ğŸ¯ æä¾›ç¬¬{step_num}æ­¥æŒ‡å¯¼"]
                    }
                
                # å¤„ç†æ–°ç›®æ ‡è§„åˆ’
                goal = result.get("goal", "ç›®æ ‡")
                why = result.get("why", "")
                timeline = result.get("timeline", "")
                milestones = result.get("milestones", [])
                first_step_data = result.get("first_step", {})
                resources = result.get("resources", [])
                tips = result.get("tips", [])
                
                print(f"   âœ“ ç›®æ ‡: {goal}")
                print(f"   âœ“ é‡Œç¨‹ç¢‘: {len(milestones)}ä¸ª")
                
                # æ„å»ºè¾“å‡º
                output_parts = [f"ğŸ¯ **ç›®æ ‡**: {goal}"]
                if why:
                    output_parts.append(f"ğŸ’¡ **åŠ¨æœº**: {why}")
                if timeline:
                    output_parts.append(f"â° **æ—¶é—´è§„åˆ’**: {timeline}")
                
                output_parts.append("\nğŸ“ **å­¦ä¹ è·¯å¾„ï¼ˆé‡Œç¨‹ç¢‘ï¼‰**:")
                for i, m in enumerate(milestones, 1):
                    milestone = m.get('milestone', '')
                    desc = m.get('description', '')
                    deadline = m.get('deadline', '')
                    actions = m.get('actions', [])
                    
                    output_parts.append(f"\n**é˜¶æ®µ{i}: {milestone}**" + (f" ({deadline})" if deadline else ""))
                    if desc:
                        output_parts.append(f"   {desc}")
                    if actions:
                        output_parts.append("   è¡ŒåŠ¨æ¸…å•:")
                        for action in actions[:3]:
                            output_parts.append(f"   âœ“ {action}")
                
                # ç¬¬ä¸€æ­¥
                output_parts.append("\nğŸš€ **ç«‹å³å¼€å§‹ï¼ˆç¬¬ä¸€æ­¥ï¼‰**:")
                if isinstance(first_step_data, dict):
                    action = first_step_data.get('action', 'å¼€å§‹è¡ŒåŠ¨')
                    time_req = first_step_data.get('time_required', '')
                    result_exp = first_step_data.get('expected_result', '')
                    
                    output_parts.append(f"   ğŸ“ {action}")
                    if time_req:
                        output_parts.append(f"   â±ï¸ é¢„è®¡è€—æ—¶: {time_req}")
                    if result_exp:
                        output_parts.append(f"   âœ¨ é¢„æœŸæˆæœ: {result_exp}")
                else:
                    output_parts.append(f"   {first_step_data}")
                
                # èµ„æº
                if resources:
                    output_parts.append("\nğŸ“š **æ¨èèµ„æº**:")
                    for res in resources[:3]:
                        output_parts.append(f"   â€¢ {res}")
                
                # å»ºè®®
                if tips:
                    output_parts.append("\nğŸ’¡ **å®ç”¨å»ºè®®**:")
                    for tip in tips[:3]:
                        output_parts.append(f"   â€¢ {tip}")
                
                final_output = "\n".join(output_parts)
                
                return {
                    "final_output": final_output,
                    "processing_steps": ["ğŸ¯ å®Œæ•´ç›®æ ‡è§„åˆ’å’Œå­¦ä¹ è·¯å¾„"]
                }
                
            except Exception as e:
                print(f"   âš ï¸ ç›®æ ‡è§„åˆ’å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§
        return {
            "final_output": "å¥½çš„ï¼è®©æˆ‘ä»¬æŠŠå¤§ç›®æ ‡æ‹†è§£æˆå°æ­¥éª¤ï¼Œä¸€æ­¥æ­¥å®ç°ï¼\n\nå»ºè®®ä»æœ€ç®€å•çš„ç¬¬ä¸€æ­¥å¼€å§‹ã€‚",
            "processing_steps": ["ğŸ¯ ç®€å•ç›®æ ‡å»ºè®®"]
        }
    
    def _reflection_guide_node(self, state: AgentState) -> Dict:
        """åæ€å¼•å¯¼èŠ‚ç‚¹ - ä½¿ç”¨ reflection_prompt"""
        print("ğŸ“ [åæ€å¼•å¯¼] ç”Ÿæˆåæ€æ¡†æ¶...")
        
        user_input = state["user_input"]
        conversation_history = state.get("conversation_history", [])
        
        # ä»å†å²ä¸­æå–æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        historical_data = ""
        if conversation_history:
            # æå–æœ€è¿‘çš„ä»»åŠ¡ã€ç›®æ ‡ç­‰ä¿¡æ¯
            recent_tasks = []
            recent_goals = []
            for turn in conversation_history[-5:]:
                if turn.get('intent') == 'task_management':
                    extracted = turn.get('extracted_data', {})
                    tasks = extracted.get('tasks', [])
                    recent_tasks.extend([t.get('title', '') for t in tasks[:3]])
                elif turn.get('intent') == 'goal_setting':
                    recent_goals.append(turn.get('user_message', '')[:50])
            
            if recent_tasks or recent_goals:
                historical_data = "æœ€è¿‘æ´»åŠ¨:\n"
                if recent_tasks:
                    historical_data += f"ä»»åŠ¡: {', '.join(recent_tasks[:5])}\n"
                if recent_goals:
                    historical_data += f"ç›®æ ‡: {', '.join(recent_goals[:3])}"
        
        if self.llm:
            try:
                # âœ… æ­£ç¡®ä½¿ç”¨ reflection_prompt
                prompt = reflection_prompt.format_messages(
                    user_input=user_input,
                    historical_data=historical_data if historical_data else "æš‚æ— å†å²æ•°æ®"
                )
                
                response = self.llm.invoke(prompt)
                result = self._parse_json_response(response.content)
                
                summary = result.get("summary", "")
                achievements = result.get("achievements", [])
                learnings = result.get("learnings", [])
                improvements = result.get("improvements", [])
                next_actions = result.get("next_actions", [])
                
                print(f"   âœ“ åæ€æ€»ç»“ç”Ÿæˆå®Œæˆ")
                
                # æ„å»ºè¾“å‡º
                output_parts = []
                
                if summary:
                    output_parts.append(f"ğŸ“Š **åæ€æ€»ç»“**\n{summary}\n")
                
                if achievements:
                    output_parts.append("âœ… **å°æˆå°±**:")
                    for a in achievements:
                        output_parts.append(f"â€¢ {a}")
                    output_parts.append("")
                
                if learnings:
                    output_parts.append("ğŸ’¡ **å­¦åˆ°çš„**:")
                    for l in learnings:
                        output_parts.append(f"â€¢ {l}")
                    output_parts.append("")
                
                if improvements:
                    output_parts.append("ğŸŒ± **å¯ä»¥æ”¹è¿›**:")
                    for imp in improvements[:2]:
                        output_parts.append(f"â€¢ {imp}")
                    output_parts.append("")
                
                if next_actions:
                    output_parts.append("ğŸš€ **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:")
                    for action in next_actions[:2]:
                        output_parts.append(f"â€¢ {action}")
                
                final_output = "\n".join(output_parts)
                
                return {
                    "final_output": final_output,
                    "processing_steps": ["ğŸ“ 4D åæ€æ¨¡å‹ç”Ÿæˆ"]
                }
                
            except Exception as e:
                print(f"   âš ï¸ åæ€å¼•å¯¼å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§
        return {
            "final_output": "è®©æˆ‘ä»¬ä¸€èµ·å›é¡¾ï¼š\n\n1. âœ… è¿™æ®µæ—¶é—´å®Œæˆäº†ä»€ä¹ˆï¼Ÿ\n2. ğŸ’¡ æœ‰ä»€ä¹ˆæ”¶è·ï¼Ÿ\n3. ğŸš€ ä¸‹ä¸€æ­¥æ€ä¹ˆåšï¼Ÿ",
            "processing_steps": ["ğŸ“ ç®€å•åæ€å¼•å¯¼"]
        }
    
    def _casual_response_node(self, state: AgentState) -> Dict:
        """é—²èŠå›åº”èŠ‚ç‚¹"""
        print("ğŸ’¬ [é—²èŠ] ç”Ÿæˆå‹å¥½å›åº”...")
        
        user_input = state["user_input"]
        conversation_history = state.get("conversation_history", [])
        
        if self.llm:
            try:
                # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
                history_text = ""
                if conversation_history:
                    recent_history = conversation_history[-3:]
                    history_text = "\n".join([
                        f"ç”¨æˆ·: {h.get('user_message', '')}\nåŠ©ç†: {h.get('assistant_message', '')[:100]}"
                        for h in recent_history
                    ])
                
                # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–å›åº”
                from langchain_core.prompts import ChatPromptTemplate
                casual_prompt = ChatPromptTemplate.from_messages([
                    ("system", """ä½ æ˜¯ LifeOS æ™ºèƒ½åŠ©ç†ï¼Œä¸€ä¸ªæ¸©æš–ã€ä¸“ä¸šã€å¯Œæœ‰åŒç†å¿ƒçš„ç”Ÿæ´»åŠ©æ‰‹ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- å‹å–„äº²åˆ‡ï¼Œåƒæœ‹å‹ä¸€æ ·äº¤æµ
- å–„äºå€¾å¬ï¼Œç†è§£ç”¨æˆ·æƒ…ç»ª
- é€‚å½“ä½¿ç”¨ emoji è®©å¯¹è¯æ›´ç”ŸåŠ¨ï¼ˆä½†ä¸è¿‡åº¦ï¼‰
- å›å¤ç®€æ´æ˜äº†ï¼Œä¸å•°å—¦
- èƒ½å¤Ÿè®°ä½å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæä¾›è¿è´¯å›å¤

æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œç”Ÿæˆæ¸©æš–ã€è‡ªç„¶ã€è´´åˆä¸Šä¸‹æ–‡çš„å›åº”ã€‚"""),
                    ("human", f"""å¯¹è¯å†å²ï¼š
{history_text if history_text else 'ï¼ˆè¿™æ˜¯ç¬¬ä¸€è½®å¯¹è¯ï¼‰'}

ç”¨æˆ·å½“å‰è¾“å…¥ï¼š{user_input}

è¯·ç”Ÿæˆä¸€ä¸ªå‹å¥½ã€è‡ªç„¶çš„å›åº”ã€‚""")
                ])
                
                response = self.llm.invoke(casual_prompt.format_messages())
                output = response.content.strip()
                
                print(f"   âœ“ ç”Ÿæˆä¸ªæ€§åŒ–å›åº”")
                
                return {
                    "final_output": output,
                    "processing_steps": ["ğŸ’¬ AI ç”Ÿæˆå‹å¥½å›åº”"]
                }
                
            except Exception as e:
                print(f"   âš ï¸ é—²èŠå›åº”å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # é™çº§å›å¤ï¼ˆåŸºäºè§„åˆ™ï¼‰
        user_input_lower = user_input.lower()
        
        if any(word in user_input_lower for word in ['ä½ å¥½', 'hi', 'hello', 'å—¨']):
            output = "ä½ å¥½ï¼æˆ‘æ˜¯ LifeOS æ™ºèƒ½åŠ©ç† ğŸ˜Š\n\næˆ‘å¯ä»¥å¸®ä½ ï¼š\nâ€¢ ğŸ“‹ ç®¡ç†ä»»åŠ¡å’Œå¾…åŠ\nâ€¢ ğŸ¯ è¿½è¸ªä¹ æƒ¯æ‰“å¡\nâ€¢ ğŸŒŸ è®¾å®šå’Œæ‹†è§£ç›®æ ‡\nâ€¢ ğŸ“ è®°å½•åæ€æ€»ç»“\nâ€¢ ğŸ’š æä¾›æƒ…ç»ªæ”¯æŒ\n\næœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°ä½ çš„å—ï¼Ÿ"
        elif any(word in user_input_lower for word in ['åŠŸèƒ½', 'èƒ½åš', 'å¯ä»¥åš', 'å¸®æˆ‘']):
            output = "æˆ‘æœ‰è¿™äº›èƒ½åŠ›ï¼š\n\n1. ğŸ“‹ **ä»»åŠ¡ç®¡ç†**ï¼šæ•´ç†å¾…åŠï¼Œæ™ºèƒ½æ’åº\n2. ğŸ¯ **ä¹ æƒ¯è¿½è¸ª**ï¼šæ‰“å¡è®°å½•ï¼Œæ•°æ®ç»Ÿè®¡\n3. ğŸŒŸ **ç›®æ ‡è§„åˆ’**ï¼šæ‹†è§£ç›®æ ‡ï¼Œåˆ¶å®šè®¡åˆ’\n4. ğŸ“ **åæ€æ€»ç»“**ï¼šå®šæœŸå›é¡¾ï¼ŒæŒç»­æ”¹è¿›\n5. ğŸ’š **æƒ…ç»ªæ”¯æŒ**ï¼šå€¾å¬ç†è§£ï¼Œæ¸©æš–é™ªä¼´\n\nè¯•è¯•å‘Šè¯‰æˆ‘ä½ ç°åœ¨æƒ³åšä»€ä¹ˆå§ï¼"
        elif any(word in user_input_lower for word in ['è°¢è°¢', 'æ„Ÿè°¢', 'thanks', 'thx']):
            output = "ä¸å®¢æ°”ï¼ğŸ˜Š å¾ˆé«˜å…´èƒ½å¸®åˆ°ä½ ã€‚\n\næœ‰å…¶ä»–éœ€è¦éšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼"
        elif any(word in user_input_lower for word in ['å†è§', 'bye', 'æ‹œæ‹œ']):
            output = "å†è§ï¼ğŸ‘‹ è®°å¾—éšæ—¶å›æ¥æ‰¾æˆ‘ï¼Œæˆ‘ä¼šä¸€ç›´åœ¨è¿™é‡Œæ”¯æŒä½ ï¼"
        else:
            output = "æˆ‘åœ¨å‘¢ï¼ğŸ˜Š æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n\nä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„ä»»åŠ¡ã€ç›®æ ‡ï¼Œæˆ–è€…åªæ˜¯èŠèŠå¤©ä¹Ÿå¯ä»¥~"
        
        return {
            "final_output": output,
            "processing_steps": ["ğŸ’¬ å‹å¥½å›åº”"]
        }
    
    def _output_generation_node(self, state: AgentState) -> Dict:
        """è¾“å‡ºç”ŸæˆèŠ‚ç‚¹ - æœ€ç»ˆæ•´åˆ"""
        print("âœ¨ [è¾“å‡ºç”Ÿæˆ] æ•´åˆæœ€ç»ˆå›å¤...")
        
        # å¦‚æœå·²æœ‰ final_outputï¼Œä¿æŒä¸å˜
        if state.get("final_output"):
            final_output = state["final_output"]
            print(f"   âœ“ ä½¿ç”¨å·²ç”Ÿæˆçš„è¾“å‡º ({len(final_output)} å­—ç¬¦)")
            return {"final_output": final_output}
        
        # å¦åˆ™æ ¹æ®æ„å›¾ç”Ÿæˆé»˜è®¤è¾“å‡º
        intent = state.get("intent", "casual_chat")
        
        if intent == "task_management":
            tasks = state.get("analyzed_tasks", [])
            if tasks:
                output = f"å¥½çš„ï¼æˆ‘å¸®ä½ æ•´ç†äº† {len(tasks)} ä¸ªä»»åŠ¡ï¼š\n\n"
                for i, task in enumerate(tasks[:5], 1):
                    output += f"{i}. {task.get('title', 'ä»»åŠ¡')}\n"
                output += "\nğŸ’¡ å»ºè®®ä»æœ€é‡è¦çš„å¼€å§‹ï¼"
            else:
                output = "æˆ‘ç†è§£äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹æ•´ç†ä»»åŠ¡å§ï¼"
        else:
            output = "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ï¼è®©æˆ‘æ¥å¸®ä½ å¤„ç†ã€‚"
        
        print(f"   âœ“ ç”Ÿæˆé»˜è®¤è¾“å‡º")
        return {"final_output": output}
    
    def _route_by_intent(self, state: AgentState) -> str:
        """æ ¹æ®æ„å›¾è·¯ç”±"""
        intent = state.get("intent", "casual_chat")
        print(f"ğŸ”€ è·¯ç”±åˆ°: {intent}")
        return intent
    
    # =========================================================================
    # æ‰§è¡Œæ–¹æ³•
    # =========================================================================
    
    def run(
        self,
        user_input: str,
        user_id: str = "default_user",
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        
        Returns:
            æ‰§è¡Œç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        print(f"{'='*60}\n")
        
        # è·å–å¯¹è¯å†å²
        conversation_history = []
        if self.conversation_manager and session_id:
            conversation_history = self.conversation_manager.get_conversation_history(
                session_id, last_n_turns=5
            )
            print(f"ğŸ“š åŠ è½½å¯¹è¯å†å²: {len(conversation_history)} è½®")
        elif self.conversation_manager:
            # åˆ›å»ºæ–°ä¼šè¯
            session_id = self.conversation_manager.create_session(user_id)
            print(f"âœ¨ åˆ›å»ºæ–°ä¼šè¯: {session_id}")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "user_input": user_input,
            "user_id": user_id,
            "session_id": session_id or "temp_session",
            "conversation_history": conversation_history,
            "intent": "",
            "confidence": 0.0,
            "context_continuation": False,
            "analyzed_tasks": [],
            "priority_analysis": {},
            "processing_steps": [],
            "final_output": "",
            "timestamp": datetime.now().isoformat()
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        try:
            result = self.workflow_app.invoke(initial_state)
            print(f"\nâœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
            print(f"ğŸ“Š å¤„ç†æ­¥éª¤: {result.get('processing_steps', [])}")
        except Exception as e:
            print(f"\nâŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            result = {
                **initial_state,
                "final_output": "æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚",
                "processing_steps": [f"é”™è¯¯: {str(e)}"]
            }
        
        # ä¿å­˜å¯¹è¯
        if self.conversation_manager and session_id:
            try:
                self.conversation_manager.add_turn(
                    session_id=session_id,
                    user_id=user_id,
                    user_message=user_input,
                    assistant_message=result.get("final_output", ""),
                    intent=result.get("intent", "unknown"),
                    intent_confidence=result.get("confidence", 0.0),
                    extracted_data={
                        "tasks": result.get("analyzed_tasks", []),
                        "steps": result.get("processing_steps", []),
                        "priority_analysis": result.get("priority_analysis", {})
                    }
                )
                print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
        
        print(f"\n{'='*60}\n")
        return result


def create_complete_workflow(
    llm_provider: str = "mock",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    model_name: str = "gpt-3.5-turbo",
    db_path: str = "lifeos_data.db"
) -> CompleteLifeOSWorkflow:
    """
    åˆ›å»ºå®Œæ•´å·¥ä½œæµå®ä¾‹
    
    Args:
        llm_provider: LLM æä¾›å•† (mock/openai/hunyuan)
        api_key: API å¯†é’¥ï¼ˆOpenAIï¼‰æˆ– SecretId:SecretKeyï¼ˆè…¾è®¯æ··å…ƒï¼‰
        base_url: API åŸºç¡€ URL
        model_name: æ¨¡å‹åç§°
        db_path: æ•°æ®åº“è·¯å¾„
    
    Returns:
        å®Œæ•´å·¥ä½œæµå®ä¾‹
    """
    llm = None
    
    if llm_provider == "hunyuan":
        # ä½¿ç”¨è…¾è®¯æ··å…ƒ SDK
        if not HUNYUAN_AVAILABLE:
            print("âŒ è…¾è®¯äº‘ SDK æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tencentcloud-sdk-python")
            print("ğŸ”„ åˆ‡æ¢åˆ° Mock æ¨¡å¼")
            llm_provider = "mock"
        else:
            try:
                from agents.hunyuan_llm import create_hunyuan_llm
                # ä»ç¯å¢ƒå˜é‡ç›´æ¥è¯»å–
                llm = create_hunyuan_llm(
                    secret_id=os.getenv("TENCENT_SECRET_ID"),
                    secret_key=os.getenv("TENCENT_SECRET_KEY"),
                    model=model_name or "hunyuan-large"
                )
                print("âœ… è…¾è®¯æ··å…ƒ LLM åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ è…¾è®¯æ··å…ƒåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("ğŸ”„ åˆ‡æ¢åˆ° Mock æ¨¡å¼")
                llm_provider = "mock"
                llm = None
    
    elif llm_provider == "openai":
        # ä½¿ç”¨ OpenAI
        try:
            llm = ChatOpenAI(
                api_key=api_key or os.getenv("OPENAI_API_KEY", "dummy"),
                base_url=base_url,
                model=model_name,
                temperature=0.7
            )
            print(f"âœ… OpenAI LLM åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {model_name})")
        except Exception as e:
            print(f"âŒ OpenAI åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ° Mock æ¨¡å¼")
            llm = None
    
    elif llm_provider == "mock":
        llm = None
        print("âœ… ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆè§„åˆ™åŒ¹é…ï¼Œæµ‹è¯•ç”¨ï¼‰")
    
    else:
        print(f"âš ï¸ æœªçŸ¥çš„ LLM æä¾›å•†: {llm_provider}")
        print("ğŸ”„ ä½¿ç”¨ Mock æ¨¡å¼")
        llm = None
    
    return CompleteLifeOSWorkflow(
        llm=llm,
        db_path=db_path,
        enable_conversation_memory=True
    )