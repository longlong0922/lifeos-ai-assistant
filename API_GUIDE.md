# LifeOS API ä½¿ç”¨æ–‡æ¡£

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from modules.lifeos_real import LifeOSRealAssistant

# åˆå§‹åŒ–åŠ©æ‰‹ï¼ˆè‡ªåŠ¨è¯»å– .env é…ç½®ï¼‰
assistant = LifeOSRealAssistant()

# ç®€å•å¯¹è¯
response = assistant.chat(
    user_id="user_001",
    user_input="ä»Šå¤©æœ‰å¥½å¤šäº‹è¦åšï¼Œæ„Ÿè§‰å¥½ç´¯"
)

# è·å–å“åº”æ–‡æœ¬
print(response["display_text"])
```

### 2. æŒ‡å®š LLM æä¾›è€…

```python
# ä½¿ç”¨è…¾è®¯æ··å…ƒ
assistant = LifeOSRealAssistant(llm_provider="hunyuan")

# ä½¿ç”¨ OpenAI
assistant = LifeOSRealAssistant(llm_provider="openai")

# ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆæµ‹è¯•ï¼‰
assistant = LifeOSRealAssistant(llm_provider="mock")
```

## æ ¸å¿ƒ API

### LifeOSRealAssistant

ä¸»æ§åˆ¶å™¨ç±»ï¼Œåè°ƒæ‰€æœ‰æ¨¡å—ã€‚

#### åˆå§‹åŒ–

```python
assistant = LifeOSRealAssistant(
    db_path="lifeos_data.db",  # æ•°æ®åº“è·¯å¾„
    llm_provider="hunyuan"      # LLM æä¾›è€…
)
```

#### chat() æ–¹æ³•

å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè¿”å›ç»“æ„åŒ–å“åº”ã€‚

**å‚æ•°ï¼š**
- `user_id` (str): ç”¨æˆ·å”¯ä¸€æ ‡è¯†
- `user_input` (str): ç”¨æˆ·è¾“å…¥æ–‡æœ¬

**è¿”å›ï¼š**
```python
{
    "success": True,                    # æ˜¯å¦æˆåŠŸ
    "mode": "action_assistant",         # æ¨¡å¼ï¼šemotion_support / action_assistant / mixed
    "response_type": "summary_card",    # å“åº”ç±»å‹
    "content": {                        # ç»“æ„åŒ–å†…å®¹
        "summary": "...",
        "priorities": [...],
        "suggested_action": {...}
    },
    "display_text": "...",             # å¯ç›´æ¥æ˜¾ç¤ºçš„æ–‡æœ¬
    "timestamp": "2024-01-13T10:30:00"
}
```

**ç¤ºä¾‹ï¼š**

```python
# æƒ…ç»ªæ”¯æŒåœºæ™¯
response = assistant.chat("user_001", "æˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¥½")
# mode = "emotion_support"

# ä»»åŠ¡å¤„ç†åœºæ™¯
response = assistant.chat("user_001", "å¸®æˆ‘æ•´ç†ä»Šå¤©çš„ä»»åŠ¡")
# mode = "action_assistant"
# response_type = "summary_card"

# ä»»åŠ¡æ‹†è§£åœºæ™¯
response = assistant.chat("user_001", "æˆ‘æƒ³å­¦ä¹  Python")
# mode = "action_assistant"
# response_type = "action_plan"
```

### è®°å¿†ç®¡ç†

è®°å½•ç”¨æˆ·åå¥½å’Œä¹ æƒ¯ã€‚

```python
from modules.memory import MemoryType

# è®°å½•åå¥½
assistant.memory_manager.remember(
    user_id="user_001",
    key="morning_productivity",
    value=True,
    memory_type=MemoryType.PREFERENCE
)

# è®°å½•ä¹ æƒ¯
assistant.memory_manager.remember(
    user_id="user_001",
    key="work_start_time",
    value="9:00",
    memory_type=MemoryType.ROUTINE
)

# è®°å½•é•¿æœŸç›®æ ‡
assistant.memory_manager.remember(
    user_id="user_001",
    key="career_goal",
    value="æˆä¸ºæ•°æ®ç§‘å­¦å®¶",
    memory_type=MemoryType.GOAL
)

# è·å–ç”¨æˆ·ç”»åƒ
profile = assistant.memory_manager.get_user_profile("user_001")
print(profile.morning_productivity)  # True
print(profile.long_term_goals)       # ["æˆä¸ºæ•°æ®ç§‘å­¦å®¶"]

# åˆ é™¤è®°å¿†
assistant.memory_manager.forget("user_001", "morning_productivity")
```

### ç›´æ¥è°ƒç”¨ LLM

```python
from modules.llm_service import call_llm

messages = [
    {"role": "system", "content": "ä½ æ˜¯ LifeOS åŠ©æ‰‹"},
    {"role": "user", "content": "å¸®æˆ‘æ•´ç†ä»»åŠ¡"}
]

response = call_llm(
    messages,
    temperature=0.7,
    max_tokens=1500
)
```

## å“åº”ç±»å‹è¯¦è§£

### 1. æƒ…ç»ªæ”¯æŒå“åº” (emotion_support)

```python
{
    "success": True,
    "mode": "emotion_support",
    "response_type": "text",
    "content": {
        "text": "å¬èµ·æ¥ä½ ç°åœ¨...",
        "options": [
            {"label": "ğŸŒ¿ è¯´è¯´è¯", "action": "continue_emotion"},
            {"label": "ğŸ“‹ å¸®æˆ‘æ•´ç†ä»»åŠ¡", "action": "switch_to_action"}
        ]
    },
    "display_text": "..."
}
```

### 2. æ™ºèƒ½æ‘˜è¦å“åº” (summary_card)

```python
{
    "success": True,
    "mode": "action_assistant",
    "response_type": "summary_card",
    "content": {
        "summary": "ç”¨æˆ·æœ‰5ä¸ªä»»åŠ¡å¾…å¤„ç†",
        "categories": ["work", "personal"],
        "highlights": ["éƒ¨åˆ†ä»»åŠ¡æ—¶é—´ç´§è¿«"],
        "priorities": [
            {
                "item": "æ˜å¤©è¦äº¤çš„æŠ¥å‘Š",
                "importance": 10,
                "urgency": 10,
                "reason": "æ˜å¤©æˆªæ­¢"
            }
        ],
        "suggested_action": {
            "desc": "å…ˆèŠ±5åˆ†é’Ÿå†™æŠ¥å‘Šæ‘˜è¦",
            "est_minutes": 5,
            "next_step": "æ‰“å¼€æ–‡æ¡£ï¼Œåˆ—å‡º3ä¸ªè¦ç‚¹"
        },
        "skip_candidates": ["ä¸ç´§æ€¥çš„é‚®ä»¶"]
    }
}
```

### 3. ä»»åŠ¡æ‹†è§£å“åº” (action_plan)

```python
{
    "success": True,
    "mode": "action_assistant",
    "response_type": "action_plan",
    "content": {
        "task": "å­¦ä¹  Python æ•°æ®åˆ†æ",
        "actions": [
            {
                "desc": "å®‰è£… Anaconda ç¯å¢ƒ",
                "est_minutes": 5,
                "type": "immediate",
                "difficulty": "easy",
                "expected_outcome": "ç¯å¢ƒå®‰è£…å®Œæˆ"
            },
            {
                "desc": "ä¸‹è½½æ•°æ®é›†",
                "est_minutes": 20,
                "type": "prep",
                "difficulty": "easy",
                "expected_outcome": "æœ‰äº†ç»ƒæ‰‹æ•°æ®"
            }
        ],
        "recommended_index": 0,
        "rationale": "æ¨èä»æœ€ç®€å•çš„ç¯å¢ƒå®‰è£…å¼€å§‹"
    }
}
```

## é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰ LLM æä¾›è€…

```python
from modules.llm_service import LLMProvider, LLMService

class CustomProvider(LLMProvider):
    def chat(self, messages, temperature=0.7, max_tokens=2000):
        # å®ç°ä½ çš„ LLM è°ƒç”¨é€»è¾‘
        response = your_llm_api(messages)
        return response

# æ³¨å†Œè‡ªå®šä¹‰æä¾›è€…
service = LLMService()
service.provider = CustomProvider()
```

### 2. æ‰¹é‡å¤„ç†

```python
user_inputs = [
    "ä»Šå¤©è¦åšä»€ä¹ˆ",
    "å¸®æˆ‘æ•´ç†ä»»åŠ¡",
    "æˆ‘æƒ³å­¦ç¼–ç¨‹"
]

results = []
for user_input in user_inputs:
    response = assistant.chat("user_001", user_input)
    results.append(response)
```

### 3. æŒä¹…åŒ–ä¼šè¯

```python
# ä¼šè¯çŠ¶æ€ä¼šè‡ªåŠ¨ä¿å­˜åœ¨æ•°æ®åº“ä¸­
# ç”¨æˆ·ä¸‹æ¬¡è®¿é—®æ—¶ä¼šè®°ä½ä¹‹å‰çš„åå¥½

# è·å–ç”¨æˆ·ç”»åƒ
profile = assistant.memory_manager.get_user_profile("user_001")

# åŸºäºç”»åƒæä¾›ä¸ªæ€§åŒ–æœåŠ¡
if profile.morning_productivity:
    print("å»ºè®®æ—©ä¸Šå¤„ç†é‡è¦ä»»åŠ¡")
```

## é”™è¯¯å¤„ç†

```python
response = assistant.chat("user_001", user_input)

if response["success"]:
    print(response["display_text"])
else:
    error_message = response.get("error", "æœªçŸ¥é”™è¯¯")
    fallback = response.get("fallback_message", "")
    print(f"é”™è¯¯: {error_message}")
    print(f"å›é€€å“åº”: {fallback}")
```

## ç¯å¢ƒé…ç½®

### .env æ–‡ä»¶

```ini
# LLM æä¾›è€…
LLM_PROVIDER="hunyuan"

# è…¾è®¯æ··å…ƒ
TENCENT_SECRET_ID="your_id"
TENCENT_SECRET_KEY="your_key"
HUNYUAN_MODEL="hunyuan-large"

# OpenAIï¼ˆå¯é€‰ï¼‰
OPENAI_API_KEY="sk-xxx"
OPENAI_MODEL="gpt-3.5-turbo"

# æ•°æ®åº“
DB_PATH="data/lifeos.db"
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ç¼“å­˜ LLM å“åº”**
   ```python
   # TODO: å®ç°å“åº”ç¼“å­˜
   ```

2. **å¼‚æ­¥å¤„ç†**
   ```python
   # TODO: ä½¿ç”¨ asyncio æå‡å¹¶å‘æ€§èƒ½
   ```

3. **æ‰¹é‡ API è°ƒç”¨**
   ```python
   # TODO: æ‰¹é‡è°ƒç”¨ LLM API
   ```

## æœ€ä½³å®è·µ

1. **åˆç†ä½¿ç”¨ Mock æ¨¡å¼**
   - å¼€å‘æ—¶ä½¿ç”¨ Mock æ¨¡å¼èŠ‚çœ API è´¹ç”¨
   - ç”Ÿäº§ç¯å¢ƒåˆ‡æ¢åˆ°çœŸå® LLM

2. **ç”¨æˆ·ç”»åƒæ›´æ–°**
   - å®šæœŸæ›´æ–°ç”¨æˆ·åå¥½
   - æ ¹æ®è¡Œä¸ºæ¨¡å¼è°ƒæ•´è®°å¿†

3. **é”™è¯¯å¤„ç†**
   - å§‹ç»ˆæ£€æŸ¥ `response["success"]`
   - æä¾›å‹å¥½çš„é”™è¯¯æç¤º

4. **éšç§ä¿æŠ¤**
   - æ•æ„Ÿä¿¡æ¯ä¸è¦å­˜å…¥è®°å¿†
   - æ”¯æŒç”¨æˆ·"å¿˜è®°æˆ‘"åŠŸèƒ½

---

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [README.md](README.md) å’Œ [UPGRADE_REPORT.md](UPGRADE_REPORT.md)
